{"name":"Wells Fargo Analytics Competition","tagline":"","body":"##*Other Contributors*\r\nKatie Duchinski (@kduchinski) and Kenzie Nash (@nashkenziem)\r\n\r\n###*Social media sites are a font of knowledge for businesses, as they act as a platform where real-world consumers can instantly provide the company with feedback and input on their experiences. The overall attitude the public takes to a business on social media- one of praise or one of complaint- is quantifiable and analyzable feedback of the companies’ performance. It is a measurable indicator of the functionality of the business, revealing whether the majority of its client base is satisfied or discontented with their services.*\r\n\r\n#Our Task \r\n**Analyze text from posts on the social media sites Twitter and Facebook regarding Banks A-D.** \r\n\r\n*We believe analyzing the frequent topics and sentiment of within these social media posts would provide the banks with useful consumer feedback.*\r\n##*Word Frequency* \r\nFirst, we determined the identities of the most frequently used words, and graphically represented these frequent words in word clouds and frequency plots. \r\n\r\n* Word clouds allowed for a clear and easily comprehensible visual representation of the topics addressed in social media posts. \r\n####Word Cloud for Entire Dataset \r\n<a href=\"http://imgur.com/i0t63I7\"><img src=\"http://i.imgur.com/i0t63I7.png\" title=\"source: imgur.com\" /></a>\r\n\r\n* Frequency plots were also included so that a numeric representation of term frequency was available, in addition to the more subjective representation of frequency seen in the word clouds. \r\n\r\n####Frequency Plot for Entire Dataset \r\n<a href=\"http://imgur.com/L6t1Xlb\"><img src=\"http://i.imgur.com/L6t1Xlb.png\" title=\"source: imgur.com\" /></a>\r\n\r\n*In each of these graphic representations, the most common topics throughout the social media text are clearly presented. Visual representations were created for the entirety of the social media text, and were then created for each of the individual banks (i.e., Banks A, B, C, and D).*\r\n\r\n\r\n##*Sentiment Analysis*\r\nTo further analyze the attitude of the consumers regarding these topics, we determined the sentiment, *i.e*. the positive, negative, or neutral connotation, within the social media posts. \r\n\r\n* The neutral terms commonly included words regarding banking and financing, such as “checking” and “online.” Regarding these frequent neutral words further indicated the topics that consumers were posting about. \r\n\r\n* Meanwhile, the positive or negative frequently used terms provided feedback as to the attitudes consumers had towards the company. The sentiment scores were then plotted and graphically represented with a box chart. \r\n\r\n*Sentiment was scored on a scale of 0-100, with 0 being associated with negative sentiment and 100 being associated with positive sentiment. Neutral words did not receive a sentiment score. The sentiment scores were averaged, and these averages were represented through a bar graph. Sentiment analysis and graphical representations of same were generated for the entire dataset, as well as for each individual bank. Further, each graph compared the sentiment of frequent terms seen between two different social media sites, Twitter and Facebook*\r\n####Average Sentiment Scores for Entire Dataset\r\n<a href=\"http://imgur.com/d8mkFuh\"><img src=\"http://i.imgur.com/d8mkFuh.png?1\" title=\"source: imgur.com\" /></a>\r\n\r\n####Boxplot Illustrating Sentiment Scores for Entire Dataset\r\n<a href=\"http://imgur.com/X3n79rf\"><img src=\"http://i.imgur.com/X3n79rf.png?1\" title=\"source: imgur.com\" /></a>\r\n\r\n#How did we execute these goals?\r\nBy using RStudio, we were able to write code allowing for the aforementioned analysis of the social media posts.\r\n\r\n<a href=\"http://imgur.com/oHrv5xe\"><img src=\"http://i.imgur.com/oHrv5xe.png\" title=\"source: imgur.com\" /></a>\r\n\r\n#Our Code \r\n##*Creation of Data Frame and Corpus*\r\nDataframe Creation\r\n\r\n    df = read.table('dataset.txt',sep=\"|\",header=T)\r\n    spellDoc(doc, wordHandler = DocSpeller(), checker = docChecker(speller), speller = getSpeller(conf), \r\n    conf = createSpellConfig())\r\n\r\n\r\nCorpus Creation\r\n\r\n\r\n    df.texts = as.data.frame(df[,ncol(df)])\r\n\r\n    colnames(df.texts) = 'FullText' \r\n\r\n\r\nRemove non-ascii characters\r\n\r\n    df.texts.clean = as.data.frame(iconv(df.texts$FullText, \"latin1\", \"ASCII\", sub=\"\"))\r\n\r\n    for(i in 1:nrow(df))\r\n\r\n\t{\r\n\r\n\tdf.texts.clean[i,] = check_spelling(df.texts.clean[i,], range = 2, assume.first.correct=TRUE,\r\n    dictionary=qdapDictionaries::GradyAugmented,parallel=TRUE,cores=parallel::detectCores()/2,n.suggests=8)\r\n\r\n\t}\r\n\r\n    colnames(df.texts.clean) = 'FullText'\r\n\r\n##*Apply TM Library* \r\n\r\n    library(tm) \r\n\r\n    docs <- Corpus(DataframeSource(df.texts.clean))\r\n\r\n##*Cleaning the text* \r\nRemove numbers and punctuation \r\n    \r\n    removeNumPunct <- function(x) gsub(\"[^[:alpha:][:space:]]*\", \"\", x)\r\n\r\n    docs <- tm_map(docs, content_transformer(removeNumPunct))\r\n\r\nRemove stopwords, from Smart English List found on CRAN, as well as hard-coding out several words \r\n\r\n    myStopwords <- c(stopwords(kind='SMART'), \"available\", \"via\",  \"twithndl\",\"twithndlBankA\",\r\n    \"twithndlBankB\",\"twithndlBankC\",\"twithndlBankD\",\"twithndlBankE\",\"INTERNET\",\r\n    \"Name\",\"PHONE\",\"ADDRESS\",\"rettwi     t\",\"Nameresp\",\"ly\", \"https\", \"bit\",\"dlvr\")\r\n\r\n    docs <- tm_map(docs, removeWords, myStopwords, lazy = T)\r\n\r\nRemove white space \r\n\r\n    docs <- tm_map(docs, stripWhitespace)\r\n\r\n##*Creation of smaller corpora for each Bank A-D* \r\n\r\nCorpora Creation \r\n\r\n    bankA.idx = which(sapply(df$FullText,function(x) grepl(\"BankA\",x)))\r\n\r\n    bankB.idx = which(sapply(df$FullText,function(x) grepl(\"BankB\",x)))\r\n\r\n    bankC.idx = which(sapply(df$FullText,function(x) grepl(\"BankC\",x)))\r\n\r\n    bankD.idx = which(sapply(df$FullText,function(x) grepl(\"BankD\",x)))\r\n\r\nAdditional cleaning within the corpora \r\n\r\n    bankA.docs = docs[bankA.idx]\r\n\r\n    bankA.docs <- tm_map(bankA.docs, removeWords, c(\"BankA\"), lazy = T)\r\n\r\n    bankB.docs = docs[bankB.idx]\r\n\r\n    bankB.docs <- tm_map(bankB.docs, removeWords, c(\"BankB\", \"twithndlBankBhelp\"), lazy = T)\r\n\r\n    bankC.docs = docs[bankC.idx]\r\n\r\n    bankC.docs <- tm_map(bankC.docs, removeWords, c(\"BankC\"), lazy = T)\r\n\r\n    bankD.docs = docs[bankD.idx]\r\n\r\n    bankD.docs <- tm_map(bankD.docs, removeWords, c(\"BankD\"), lazy = T)\r\n\r\nCreate term document matrices for each bank\r\n\r\n     tdm <- TermDocumentMatrix(docs, control = list(wordLengths = c(1, Inf)))\r\n\r\n    (freq.terms <- findFreqTerms(tdm, lowfreq = 5000))\r\n\r\n    tdmA <- TermDocumentMatrix(bankA.docs, control = list(wordLengths = c(1, Inf)))\r\n\r\n    (freq.terms <- findFreqTerms(tdmA, lowfreq = 1000))\r\n\r\n    tdmB <- TermDocumentMatrix(bankB.docs, control = list(wordLengths = c(1, Inf)))\r\n\r\n    (freq.terms <- findFreqTerms(tdmB, lowfreq = 1000))\r\n\r\n    tdmC <- TermDocumentMatrix(bankC.docs, control = list(wordLengths = c(1, Inf)))\r\n\r\n    (freq.terms <- findFreqTerms(tdmC, lowfreq = 1000))\r\n\r\n    tdmD <- TermDocumentMatrix(bankD.docs, control = list(wordLengths = c(1, Inf)))\r\n\r\n    (freq.terms <- findFreqTerms(tdmD, lowfreq = 1000))\r\n\r\n\r\n\r\n##*Sentiment Analysis* \r\nApply dictionaries of positive and negative words\r\n\r\n    pos <- scan('positive-words.txt',what='character',comment.char=';')\r\n\r\n    neg <- scan('negative-words.txt',what='character',comment.char=';')\r\n\r\nApply text to positive and negative dictionaries\r\n\r\n    score.sentiment = function(sentences, pos.words, neg.words, .progress='none')\r\n\r\n    {\r\n\r\n    require(plyr)\r\n\r\n    require(stringr)\r\n\r\n    scores = laply(sentences, function(sentence, pos.words, neg.words) {\r\n\r\n    word.list = str_split(sentence, '\\\\s+')\r\n\r\n    words = unlist(word.list)  \r\n\r\n    # compare our words to the dictionaries of positive & negative terms\r\n\r\n    pos.matches = match(words, pos.words)\r\n\r\n    neg.matches = match(words, neg.words)  \r\n\r\n    # match() returns the position of the matched term or NA\r\n\r\n    # we just want a TRUE/FALSE:\r\n\r\n    pos.matches = !is.na(pos.matches)\r\n\r\n    neg.matches = !is.na(neg.matches)\r\n\r\n    # and conveniently enough, TRUE/FALSE will be treated as 1/0 by sum():\r\n\r\n    score = sum(pos.matches) - sum(neg.matches)\r\n\r\n    return(score)\r\n\r\n    }, pos.words, neg.words, .progress=.progress )\r\n\r\n    scores.df = data.frame(score=scores, text=sentences)\r\n\r\n    return(scores.df)\r\n\r\n    }\r\n\r\nSentiment score for Bank A; repeat this for each bank \r\n\r\n    df.sentiment = df[bankA.idx,]\r\n\r\n    scores = score.sentiment(df.sentiment$FullText, pos, neg, .progress='text')\r\n\r\n    scores$very.pos = as.numeric(scores$score >= 2)\r\n\r\n    scores$very.neg = as.numeric(scores$score <= -2)\r\n\r\n    numpos = sum(scores$very.pos)\r\n\r\n    numneg = sum(scores$very.neg)\r\n\r\n    global_score = round( 100 * numpos / (numpos + numneg) )\r\n\r\n    scores$mediatype = df.sentiment$MediaType\r\n\r\n    cols = c(\"#7CAE00\", \"#00BFC4\")\r\n\r\n    names(cols) = c(\"twitter\", \"facebook\")\r\n\r\nCreate boxplot for Bank A; repeat for each Bank \r\n\r\n    library(ggplot2)\r\n\r\n    ggplot(scores, aes(x=mediatype, y=score, group=mediatype)) +\r\n\r\n    geom_boxplot(aes(fill=mediatype)) +\r\n\r\n    scale_fill_manual(values=cols) +\r\n\r\n    geom_jitter(colour=\"gray40\",position=position_jitter(width=0.2), alpha=0.3) +\r\n\r\n    labs(title = \"Media Type's Sentiment Scores\") + \r\n\r\n    xlab('Media Type') + ylab('Sentiment Score')\r\n\r\nCreate bar graph of average sentiment scores for Bank A; repeat for each bank \r\n\r\n    meanscore = tapply(scores$score, scores$mediatype, mean)\r\n\r\n    df.plot = data.frame(mediatype=names(meanscore), meanscore=meanscore)\r\n\r\n    df.plot$mediatypes <- reorder(df.plot$mediatype, df.plot$meanscore)\r\n\r\n    ggplot(df.plot, aes(x = factor(mediatypes), y = meanscore, fill=mediatypes)) +\r\n\r\n    geom_bar(stat=\"identity\") +\r\n\r\n    scale_fill_manual(values=cols[order(df.plot$meanscore)]) +\r\n\r\n    labs(title = \"Average Sentiment Score\") + \r\n\r\n    xlab('Media Type') + ylab('Average Score')\r\n\r\n    # barplot of average very positive\r\n\r\n    mediatype_pos = ddply(scores, .(mediatype), summarise, mean_pos=mean(very.pos))\r\n\r\n    mediatype_pos$mediatypes <- reorder(mediatype_pos$mediatype, mediatype_pos$mean_pos)\r\n\r\n    ggplot(mediatype_pos, aes(x = factor(mediatype), y = mean_pos, fill=mediatype)) +\r\n\r\n    geom_bar(stat=\"identity\") +\r\n \r\n    scale_fill_manual(values=cols[order(df.plot$meanscore)]) +\r\n\r\n    labs(title = \"Average Very Negative Sentiment Score\") + \r\n\r\n    xlab('Media Type') + ylab('Average Score')\r\n\r\n    mediatype_neg = ddply(scores, .(mediatype), summarise, mean_neg=mean(very.neg))\r\n\r\n    mediatype_neg$mediatypes <- reorder(mediatype_neg$mediatype, mediatype_neg$mean_neg)\r\n\r\n    ggplot(mediatype_neg, aes(x = factor(mediatype), y = mean_neg, fill=mediatype)) +\r\n\r\n    geom_bar(stat=\"identity\") +\r\n\r\n    scale_fill_manual(values=cols[order(df.plot$meanscore)]) +\r\n\r\n    labs(title = \"Average Very Negative Sentiment Score\") + \r\n\r\n    xlab('Media Type') + ylab('Average Score')\r\n\r\n\r\n##*\"Spell Check\" through correction of misspelled, frequently used words* \r\n\r\nDefine, once only \r\n\r\n    dictCorpusA = VCorpus(VectorSource(dimnames(tdmA)$Terms))\r\n\r\n    dictCorpusB = VCorpus(VectorSource(dimnames(tdmB)$Terms))\r\n\r\n    dictCorpusC = VCorpus(VectorSource(dimnames(tdmC)$Terms))\r\n\r\n    dictCorpusD = VCorpus(VectorSource(dimnames(tdmD)$Terms))\r\n\r\n    dictCorpus = VCorpus(VectorSource(dimnames(tdm)$Terms))\r\n\r\n    stemCompletion_mod <- function(x,dict=dictCorpus) {\r\n\r\n    paste(stemCompletion(unlist(strsplit(as.character(x),\" \")),dict, type=\"longest\"),sep=\" \",collapse=\" \")\r\n\r\n    }\r\n\r\nManually replace incorrect terms; here, term replacement for Bank A is shown. \r\n\r\n    terms = dimnames(tdmA)$Terms\r\n\r\n    for (i in 1:length(terms)) {\r\n\r\n\tif (terms[i] == 'appli') {\r\n\r\n\t\tterms[i] = 'application'\r\n\r\n\t} else if (terms[i] == 'busi') {\r\n\r\n\t\tterms[i] = 'business'\r\n\r\n\t}else if (terms[i] == 'financi') {\r\n\r\n\t\tterms[i] = 'financial'\r\n\r\n\t}else if (terms[i] == 'manag') {\r\n\r\n\t\tterms[i] = 'manage'\r\n\r\n\t}else if (terms[i] == 'bankac') {\r\n\r\n\t\tterms[i] = 'banka'\r\n\r\n\t}else if (terms[i] == 'charg') {\r\n\r\n\t\tterms[i] = 'charge'\r\n\r\n\t}else if (terms[i] == 'famili') {\r\n\r\n\t\tterms[i] = 'families'\r\n\r\n\t}else if (terms[i] == 'compani') {\r\n\r\n\t\tterms[i] = 'companies'\r\n\r\n\t}else if (terms[i] == 'fuck') {\r\n\r\n\t\tterms[i] = 'f***'\r\n\r\n\t}else if (terms[i] == 'donat') {\r\n\r\n\t\tterms[i] = 'donation'\r\n\r\n\t}else if (terms[i] == 'mortgag') {\r\n\r\n\t\tterms[i] = 'mortgage'\r\n\r\n\t}else if (terms[i] == 'peopl') {\r\n\r\n\t\tterms[i] = 'people'\r\n\r\n\t}else if (terms[i] == 'servic') {\r\n\r\n\t\tterms[i] = 'service'\r\n\r\n\t}else if (terms[i] == 'dlvr') {\r\n\r\n\t\tterms[i] = ''\r\n\r\n\t}else if (terms[i] == 'nameresp') {\r\n\r\n\t\tterms[i] = ''\r\n\r\n\t}else if (terms[i] == 'bit') {\r\n\r\n\t\tterms[i] = ''\r\n\r\n\t}else if (terms[i] == 'bank') {\r\n\r\n\t\tterms[i] = ''\r\n\r\n\t}\r\n\r\n\r\n     }\r\n\r\nTerm replacement for Bank B.\r\n\r\n    terms = dimnames(tdmB)$Terms\r\n\r\n    for (i in 1:length(terms)) {\r\n\r\n\tif (terms[i] == 'bank'){\r\n\r\n\t\tterms[i] = ''\r\n\r\n\t} else if (terms[i] == 'busi') {\r\n\r\n\t\tterms[i] = 'business'\r\n\r\n\t}else if (terms[i] == 'financi') {\r\n\r\n\t\tterms[i] = 'financial'\r\n\r\n\t}else if (terms[i] == 'manag') {\r\n\r\n\t\tterms[i] = 'manage'\r\n\r\n\t}else if (terms[i] == 'charg') {\r\n\r\n\t\tterms[i] = 'charge'\r\n\r\n\t}else if (terms[i] == 'corpor') {\r\n\r\n\t\tterms[i] = 'corporate'\r\n\r\n\t}else if (terms[i] == 'feedbankb') {\r\n\r\n\t\tterms[i] = ''\r\n\r\n\t}else if (terms[i] == 'fuck') {\r\n\r\n\t\tterms[i] = 'f***'\r\n\r\n\t}else if (terms[i] == 'mortgag') {\r\n\r\n\t\tterms[i] = 'mortgage'\r\n\r\n\t}else if (terms[i] == 'peopl') {\r\n\r\n\t\tterms[i] = 'people'\r\n\r\n\t}else if (terms[i] == 'servic') {\r\n\r\n\t\tterms[i] = 'service'\r\n\r\n\t}else if (terms[i] == 'dlvr') {\r\n\r\n\t\tterms[i] = ''\r\n\r\n\t}else if (terms[i] == 'nameresp') {\r\n\r\n\t\tterms[i] = ''\r\n\r\n\t}else if (terms[i] == 'bit') {\r\n\r\n\t\tterms[i] = ''\r\n\r\n\t}else if (terms[i] == 'tt') {\r\n\r\n\t\tterms[i] = ''\r\n\r\n\t}else if (terms[i] == 'ift') {\r\n\r\n\t\tterms[i] = ''\r\n\r\n\t}else if (terms[i] == 'compani') {\r\n\r\n\t\tterms[i] = 'companies'\r\n\r\n\t}else if (terms[i] == 'locat') {\r\n\r\n\t\tterms[i] = 'location'\r\n\r\n\t}\r\n\r\n    }\r\n\r\n    dimnames(tdmB)$Terms = terms\r\n\r\nTerm replacement for Bank C.\r\n\r\n    terms = dimnames(tdmC)$Terms\r\n\r\n    for (i in 1:length(terms)) {\r\n\r\n\tif (terms[i] == 'busi') {\r\n\r\n\t\tterms[i] = 'business'\r\n\r\n\t}else if (terms[i] == 'bit') {\r\n\r\n\t\tterms[i] = ''\r\n\r\n\t}else if (terms[i] == 'ift') {\r\n\r\n\t\tterms[i] = ''\r\n\r\n\t}else if (terms[i] == 'tt') {\r\n\r\n\t\tterms[i] = ''\r\n\r\n\t}else if (terms[i] == 'bank') {\r\n\r\n\t\tterms[i] = ''\r\n\r\n\t}else if (terms[i] == 'ank') {\r\n\r\n\t\tterms[i] = ''\r\n\r\n\t}else if (terms[i] == 'nameresp') {\r\n\r\n\t\tterms[i] = ''\r\n\r\n\t}else if (terms[i] == 'fb') {\r\n\r\n\t\tterms[i] = ''\r\n\r\n\t}else if (terms[i] == 'gl') {\r\n\r\n\t\tterms[i] = ''\r\n\r\n\t}else if (terms[i] == 'goo') {\r\n\r\n\t\tterms[i] = ''\r\n\r\n\t}else if (terms[i] == 'ow') {\r\n\r\n\t\tterms[i] = ''\r\n\r\n\t}else if (terms[i] == 'compani') {\r\n\r\n\t\tterms[i] = 'companies'\r\n\r\n\t}else if (terms[i] == 'charg') {\r\n\r\n\t\tterms[i] = 'charge'\r\n\r\n\t}else if (terms[i] == 'chang') {\r\n\r\n\t\tterms[i] = 'change'\r\n\r\n\t}else if (terms[i] == 'financi') {\r\n\r\n\t\tterms[i] = 'financial'\r\n\r\n\t}else if (terms[i] == 'reiter') {\r\n\r\n\t\tterms[i] = 'reiterate'\r\n\r\n\t}\r\n\r\n    }\r\n\r\n    dimnames(tdmC)$Terms = terms\r\n\r\nTerm Replacement for Bank D \r\n\r\n    terms = dimnames(tdmD)$Terms\r\n\r\n    for (i in 1:length(terms)) {\r\n\r\n\tif (terms[i] == 'appli') {\r\n\r\n\t\tterms[i] = 'application'\r\n\r\n\t}else if (terms[i] == 'busi') {\r\n\r\n\t\tterms[i] = 'business'\r\n\r\n\t}else if (terms[i] == 'https') {\r\n\r\n\t\tterms[i] = ''\r\n\r\n\t}else if (terms[i] == 'manag') {\r\n\r\n\t\tterms[i] = 'manage'\r\n\r\n\t}else if (terms[i] == 'servic') {\r\n\r\n\t\tterms[i] = 'service'\r\n\r\n\t}else if (terms[i] == 'advis') {\r\n\r\n\t\tterms[i] = 'advise'\r\n\r\n\t}else if (terms[i] == 'mortgag') {\r\n\r\n\t\tterms[i] = 'mortgage'\r\n\r\n\t}else if (terms[i] == 'reiter') {\r\n\r\n\t\tterms[i] = 'reiterate'\r\n\r\n\t}else if (terms[i] == 'ift') {\r\n\r\n\t\tterms[i] = ''\r\n\r\n\t}else if (terms[i] == 'tt') {\r\n\r\n\t\tterms[i] = ''\r\n\r\n\t}else if (terms[i] == 'financi') {\r\n\r\n\t\tterms[i] = 'financial'\r\n\r\n\t}else if (terms[i] == 'fb') {\r\n\r\n\t\tterms[i] = ''\r\n\r\n\t}else if (terms[i] == 'compani') {\r\n\r\n\t\tterms[i] = 'companies'\r\n\r\n\t}else if (terms[i] == 'bank') {\r\n\r\n\t\tterms[i] = ''\r\n\r\n\t}else if (terms[i] == 'charg') {\r\n\r\n\t\tterms[i] = 'charge'\r\n\r\n\t}else if (terms[i] == 'peopl') {\r\n\r\n\t\tterms[i] = 'people'\r\n\r\n\t}else if (terms[i] == 'rais') {\r\n\r\n\t\tterms[i] = 'raise'\r\n\r\n\t}\r\n\r\n    }\r\n\r\n\r\n##*Create Term Frequency Graph* \r\nHere, creation of graph for Bank A is shown. Repeat for every bank. \r\n\r\n    library(slam)\r\n\r\n    bankA.docs <- rollup(bankA.docs, 2, na.rm=TRUE, FUN = sum)\r\n\r\n    liblibterm.freqA <- rowSums(as.matrix(tdmA))\r\n\r\n    term.freqA <- subset(term.freqA, term.freqA >= 1000)\r\n\r\n    df.A <- data.frame(term = names(term.freqA), freq = term.freqA)\r\n\r\n    library(slam)\r\n\r\n    (freq.terms <- findFreqTerms(tdmA, lowfreq = 1000))      \r\n\r\n    bankA.docs <- rollup(bankA.docs, 2, na.rm=TRUE, FUN = sum)\r\n\r\n    term.freqA <- rowSums(as.matrix(tdmA))\r\n\r\n    term.freqA <- subset(term.freqA, term.freqA >= 1000)\r\n\r\n    df.A <- data.frame(term = names(term.freqA), freq = term.freqA)\r\n\r\n    tdmA <- rollup(tdmA, 2, na.rm=TRUE, FUN = sum)\r\n\r\n    term.freqA <- rowSums(as.matrix(tdmA))\r\n\r\n    term.freqA <- subset(term.freqA, term.freqA >= 1000)\r\n\r\n    df.A <- data.frame(term = names(term.freqA), freq = term.freqA)\r\n\r\n##*Creation of Word Cloud* \r\nHere, creation of word cloud for Bank A is shown. Repeat for each bank. \r\n    \r\n    mA <- as.matrix(tdmA)\r\n\r\n    word.freq <- sort(rowSums(mA), decreasing = T)\r\n\r\n    pal <- brewer.pal(9, \"BuGn\")\r\n\r\n    pal <- pal[-(1:4)]\r\n\r\n    library(wordcloud)\r\n\r\n    wordcloud(words = names(word.freq), freq = word.freq, min.freq=700, random.order=F, random.color=F, colors = pal)\r\n\r\n\r\n#Results\r\n##*Entire Dataset*\r\n###**Word Frequency**\r\nMost frequently discussed words, related to banking, from the entire dataset:\r\n\r\n1. Credit \r\n2. Check \r\n3. Card\r\n4. Money \r\n5. Finance\r\n6. Custom \r\n7. Call \r\n8. Share \r\n\r\n***insert word cloud/ggplot***\r\n\r\n###**Sentiment Analysis**\r\nAverage Sentiment Score of Entire Dataset: 62\r\n\r\n*Facebook was slightly more positive as Twitter, and Facebook had notably more variability and more instances of strong sentiment.*\r\n\r\n##*Bank A* \r\n###**Word Frequency**\r\nMost frequently discussed words, related to banking, from the social media posts about Bank A:\r\n\r\n1. Time \r\n2. Support\r\n3. Share \r\n4. Phone \r\n5. People \r\n6. Open \r\n7. Mortgage \r\n8. Money \r\n9. Families \r\n10. Donation \r\n11. Deposit \r\n12. Close \r\n13. Check \r\n14. Center \r\n####Frequency Plot, Bank A\r\n<a href=\"http://imgur.com/zXoK8vL\"><img src=\"http://i.imgur.com/zXoK8vL.png\" title=\"source: imgur.com\" /></a>\r\n##### Word Cloud, Bank A\r\n<a href=\"http://imgur.com/mMAEZHI\"><img src=\"http://i.imgur.com/mMAEZHI.png\" title=\"source: imgur.com\" /></a>\r\n\r\n###**Sentiment Analysis**\r\nAverage Sentiment Score of Entire Dataset: 65\r\n\r\n*Facebook was over twice as positive as Twitter, and Facebook had much more variability within and instances of strong sentiment.* \r\n####Average Sentiment Scores, Bank A\r\n<a href=\"http://imgur.com/YPlwQKU\"><img src=\"http://i.imgur.com/YPlwQKU.png\" title=\"source: imgur.com\" /></a>\r\n####Boxplot Illustrating Sentiment Scores, Bank A \r\n<a href=\"http://imgur.com/cmRPy4A\"><img src=\"http://i.imgur.com/cmRPy4A.png\" title=\"source: imgur.com\" /></a>\r\n\r\n##*Bank B* \r\n###**Word Frequency**\r\nMost frequently discussed words, related to banking, from the social media posts about Bank B:\r\n\r\n1. Time \r\n2. Team \r\n3. Stadium \r\n4. Service\r\n5. People\r\n6. Open \r\n7. Mortgage\r\n8. Money and Financial \r\n9. Loan \r\n10. Great/Good\r\n11. Fee\r\n12. Deposit\r\n13. Charge \r\n14. Cash \r\n15. Business/Corporate \r\n16. ATM \r\n17. Account \r\n\r\n####Frequency Plot, Bank B \r\n<a href=\"http://imgur.com/06WV2J9\"><img src=\"http://i.imgur.com/06WV2J9.png\" title=\"source: imgur.com\" /></a>\r\n####Word Cloud, Bank B\r\n<a href=\"http://imgur.com/9sOdzGM\"><img src=\"http://i.imgur.com/9sOdzGM.png\" title=\"source: imgur.com\" /></a>\r\n\r\n###**Sentiment Analysis**\r\nAverage Sentiment Score of Entire Dataset: 55\r\n\r\n*Facebook was slightly more positive as Twitter, and Facebook had slightly more variability within and instances of strong sentiment.* \r\n####Average Sentiment Scores, Bank B\r\n<a href=\"http://imgur.com/Uf7DhrY\"><img src=\"http://i.imgur.com/Uf7DhrY.png\" title=\"source: imgur.com\" /></a>\r\n####Boxplot Illustrating Sentiment Scores, Bank B\r\n<a href=\"http://imgur.com/wayNWPP\"><img src=\"http://i.imgur.com/wayNWPP.png\" title=\"source: imgur.com\" /></a>\r\n\r\n##*Bank C* \r\n###**Word Frequency**\r\nMost frequently discussed words, related to banking, from the social media posts about Bank C:\r\n\r\n1. Rate\r\n2. Pay \r\n3. Credit \r\n4. Card \r\n5. Buy\r\n6. Business \r\n7. Account \r\n\r\n####Frequency Plot, Bank C\r\n<a href=\"http://imgur.com/x0EnllK\"><img src=\"http://i.imgur.com/x0EnllK.png\" title=\"source: imgur.com\" /></a>\r\n####Word Cloud, Bank C \r\n<a href=\"http://imgur.com/B79ZjH6\"><img src=\"http://i.imgur.com/B79ZjH6.png\" title=\"source: imgur.com\" /></a>\r\n\r\n###**Sentiment Analysis**\r\nAverage Sentiment Score of Entire Dataset: 52\r\n\r\n*Facebook was over three times more positive than Twitter, and Facebook had more variability within and instances of strong sentiment.*\r\n####Average Sentiment Scores, Bank C\r\n<a href=\"http://imgur.com/nSiQJDK\"><img src=\"http://i.imgur.com/nSiQJDK.png\" title=\"source: imgur.com\" /></a>\r\n####Boxplot Illustrating Sentiment Scores, Bank C\r\n<a href=\"http://imgur.com/yZ5L1ei\"><img src=\"http://i.imgur.com/yZ5L1ei.png\" title=\"source: imgur.com\" /></a>\r\n\r\n##*Bank D* \r\n###**Word Frequency**\r\nMost frequently discussed words, related to banking, from the social media posts about Bank D:\r\n\r\n1. Small\r\n2. Program \r\n3. Mission \r\n4. Manage \r\n5. Learn \r\n6. Application \r\n7. Business\r\n\r\n####Frequency Plot, Bank D\r\n<a href=\"http://imgur.com/3MeTEd1\"><img src=\"http://i.imgur.com/3MeTEd1.png\" title=\"source: imgur.com\" /></a>\r\n####Word Cloud, Bank D\r\n<a href=\"http://imgur.com/eNoAlVJ\"><img src=\"http://i.imgur.com/eNoAlVJ.png\" title=\"source: imgur.com\" /></a>\r\n\r\n###**Sentiment Analysis**\r\nAverage Sentiment Score of Entire Dataset: 62\r\n\r\n*Facebook was slightly more positive as Twitter, and Facebook had slightly more variability within and instances of strong sentiment.*\r\n\r\n####Average Sentiment Scores, Bank D\r\n<a href=\"http://imgur.com/j74lcbn\"><img src=\"http://i.imgur.com/j74lcbn.png\" title=\"source: imgur.com\" /></a>\r\n####Boxplot Illustrating Sentiment Scores, Bank D\r\n<a href=\"http://imgur.com/VsC9Oly\"><img src=\"http://i.imgur.com/VsC9Oly.png\" title=\"source: imgur.com\" /></a>\r\n\r\n###*As visually evidenced by the sentiment bar graphs, it would appear that positive or negative sentiment is correlated to social media platform. In each of the individual banks, as well as the entire dataset, Facebook was associated with more positive posts, while Twitter was conversely associated with more negative posts. This finding implies that, in order to effectively interpret data from social media posts, data should be collected from multiple platforms to minimize sampling bias.*\r\n\r\n###*Additionally, the average sentiment score between all of the banks was a 62. Of the banks of interest, Banks A-D, only Bank A had a higher mean sentiment score than 62. Banks B-D all scored below 60, with scores of 55, 52, and 53, respectively. These data suggest than Bank A is associated with exceptionally positive social media posts, while Banks B-D are associated with more negative sentiment scores below average. Overall, it would appear that there are differences- although slight- that may be seen through analyzing the positive or negative connotations of social media posts of competing businesses.*\r\n\r\n\r\n","google":"","note":"Don't delete this file! It's used internally to help with page regeneration."}